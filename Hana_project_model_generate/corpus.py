# -*- coding: utf-8 -*-
"""하나corpus

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/133zBYRmPNV0UDCKujkPcSKqvDw4rgV4d
"""

import torch
import os

os.chdir('/content/drive/MyDrive/Hana_project')

import time
from tqdm import tqdm
from functools import partial
from filelock import FileLock
from dataclasses import dataclass
from typing import List, Optional
from multiprocessing import Pool, cpu_count
from transformers import PreTrainedTokenizer
from torch.utils.data.dataset import Dataset
import utill
import pandas as pd
from arguments import TrainArguments

@dataclass
class ClassificationExample:
    text_a: str
    text_b: Optional[str] = None
    label: Optional[str] = None

@dataclass
class ClassificationFeatures:
    input_ids: List[int]
    attention_mask: Optional[List[int]] = None
    token_type_ids: Optional[List[int]] = None
    label: Optional[int] = None

class ClassCorpus:
    def __init__(self):
      self.train_file = "Hana_power_v1_train_class.csv"
      
    def get_examples(self, corpus_dir, mode):
        corpus_fpath = os.path.join(corpus_dir, self.train_file)
        QA_df = pd.read_csv(corpus_fpath, encoding = 'cp949')

        examples = []
        
        for i in range(len(QA_df)):

          content = QA_df.loc[i,:]

          examples.append(ClassificationExample(text_a=content['question_text'], text_b=None, label=content['class']))

        return examples

    def get_labels(self):
       
        return ['수익권과 수익증권','투자신탁재산','판매 및 환매','기타']

    @property
    def num_labels(self):
        return len(self.get_labels())

def _convert_examples_to_classification_features(
        examples: List[ClassificationExample],
        tokenizer: PreTrainedTokenizer,
        args: TrainArguments,
        label_list: List[str],
):
    label_map = {label: i for i, label in enumerate(label_list)}
    labels = [label_map[example.label] for example in examples]

    start = time.time()
    batch_encoding = tokenizer(
        [(example.text_a, example.text_b) for example in examples],
        max_length=args.max_seq_length,
        padding="max_length",
        truncation=True,
    )
   
    features = []
    for i in range(len(examples)):
        inputs = {k: batch_encoding[k][i] for k in batch_encoding}
        feature = ClassificationFeatures(**inputs, label=labels[i])
        features.append(feature)

    return features


class ClassificationDataset(Dataset):

    def __init__(
            self,
            args: TrainArguments,
            tokenizer: PreTrainedTokenizer,
            corpus,
            mode: Optional[str] = "train",
            convert_examples_to_features_fn=_convert_examples_to_classification_features,
    ):
        if corpus is not None:
            self.corpus = corpus
        else:
            raise KeyError("corpus is not valid")
        if not mode in ["train", "val", "test"]:
            raise KeyError(f"mode({mode}) is not a valid split name")
        # Load data features from cache or dataset file
        cached_features_file = os.path.join(
            args.downstream_corpus_root_dir,
            args.downstream_corpus_name,
            "cached_{}_{}_{}_{}_{}".format(
                mode,
                tokenizer.__class__.__name__,
                str(args.max_seq_length),
                args.downstream_corpus_name,
                args.downstream_task_name,
            ),
        )

    def __len__(self):
        return len(self.features)

    def __getitem__(self, i):
        return self.features[i]

    def get_labels(self):
        return self.corpus.get_labels()

@dataclass
class QAExample:
    question_text: str
    context_text: str
    answer_text: str
    start_position_character: Optional[int] = None

class QACorpus:

    def __init__(self):
        pass

    def get_examples(self, corpus_dir, mode):
        """
        :return: List[QAExample]
        """
        raise NotImplementedError

class QACorpus(QACorpus):
    
    def __init__(self):
        super().__init__()
        self.train_file = "Hana_power_v1_train.csv"
        self.val_file = "Hana_power_v1_dev.csv"
  
    def get_examples(self, corpus_dir, mode):
        examples = []
        if mode == "train":
            corpus_fpath = os.path.join(corpus_dir, self.train_file)
        elif mode == "val":
            corpus_fpath = os.path.join(corpus_dir, self.val_file)
        else:
            raise KeyError(f"mode({mode}) is not a valid split name")

        QA_df = pd.read_csv(corpus_fpath, encoding = 'cp949')

        for i in range(len(QA_df)):
          content = QA_df.loc[i,:]
          example = QAExample(question_text=content['question_text'],
                              context_text=content['content'],
                              answer_text=content['answer_text'],
                              start_position_character=content['start_position_character'],
                              )
          examples.append(example)

        return examples

@dataclass
class QAFeatures:
    input_ids: List[int]
    attention_mask: List[int]
    token_type_ids: List[int]
    # start_positions : 지문상 시작 토큰 위치 (wordpiece 토큰 기준)
    start_positions: int
    # end_position : 지문상 끝 토큰 위치 (wordpiece 토큰 기준)
    end_positions: int


def _squad_convert_example_to_features_init(tokenizer_for_convert):
    global tokenizer
    tokenizer = tokenizer_for_convert


def _is_whitespace(c):
    if c == " " or c == "\t" or c == "\r" or c == "\n" or ord(c) == 0x202F:
        return True
    return False


def _whitespace_tokenize(text):
    """Runs basic whitespace cleaning and splitting on a piece of text."""
    text = text.strip()
    if not text:
        return []
    tokens = text.split()
    return tokens


def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer, orig_answer_text):
    """Returns tokenized answer spans that better match the annotated answer."""
    tok_answer_text = " ".join(tokenizer.tokenize(orig_answer_text))
    for new_start in range(input_start, input_end + 1):
        for new_end in range(input_end, new_start - 1, -1):
            text_span = " ".join(doc_tokens[new_start : (new_end + 1)])
            if text_span == tok_answer_text:
                return new_start, new_end
    return input_start, input_end


def _squad_convert_example_to_features(example, max_seq_length, doc_stride, max_query_length):
    features = []

    doc_tokens, char_to_word_offset = [], []
    prev_is_whitespace = True

    # Split on whitespace so that different tokens may be attributed to their original position.
    for c in example.context_text:
        if _is_whitespace(c):
            prev_is_whitespace = True
        else:
            if prev_is_whitespace:
                doc_tokens.append(c)
            else:
                doc_tokens[-1] += c
            prev_is_whitespace = False
        char_to_word_offset.append(len(doc_tokens) - 1)

    # Get start and end position
    start_position = char_to_word_offset[example.start_position_character]
    end_position = char_to_word_offset[
        min(example.start_position_character + len(example.answer_text) - 1, len(char_to_word_offset) - 1)
    ]

    # If the answer cannot be found in the text, then skip this example.
    actual_text = " ".join(doc_tokens[start_position:(end_position + 1)])
    cleaned_answer_text = " ".join(_whitespace_tokenize(example.answer_text))
  
    if actual_text.find(cleaned_answer_text) == -1:
        return []

    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)

    tok_start_position = orig_to_tok_index[start_position]
    if end_position < len(doc_tokens) - 1:
        tok_end_position = orig_to_tok_index[end_position + 1] - 1
    else:
        tok_end_position = len(doc_tokens) - 1

    (tok_start_position, tok_end_position) = _improve_answer_span(
        all_doc_tokens, tok_start_position, tok_end_position, tokenizer, example.answer_text
    )

    spans = []

    truncated_query = tokenizer.encode(
        example.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length
    )
    sequence_added_tokens = (
        tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1
        if "roberta" in str(type(tokenizer)) or "camembert" in str(type(tokenizer))
        else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    )
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    #
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        encoded_dict = tokenizer.encode_plus(
            truncated_query if tokenizer.padding_side == "right" else span_doc_tokens,
            span_doc_tokens if tokenizer.padding_side == "right" else truncated_query,
            truncation="only_second" if tokenizer.padding_side == "right" else "only_first",
            padding="max_length",
            max_length=max_seq_length,
            return_overflowing_tokens=True,
            stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens,
            return_token_type_ids=True,
        )

        paragraph_len = min(
            len(all_doc_tokens) - len(spans) * doc_stride,
            max_seq_length - len(truncated_query) - sequence_pair_added_tokens,
        )

        encoded_dict["start"] = len(spans) * doc_stride
        encoded_dict["length"] = paragraph_len

        spans.append(encoded_dict)

        if "overflowing_tokens" not in encoded_dict or (
            "overflowing_tokens" in encoded_dict and len(encoded_dict["overflowing_tokens"]) == 0
        ):
            break
    
        span_doc_tokens = encoded_dict["overflowing_tokens"]

    for span in spans:
        # Identify the position of the CLS token
        cls_index = span["input_ids"].index(tokenizer.cls_token_id)
        # For training, if our document chunk does not contain an annotation
        # we throw it out, since there is nothing to predict.
        doc_start = span["start"]
        doc_end = span["start"] + span["length"] - 1
        out_of_span = False

        if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
            out_of_span = True

        if out_of_span:
            start_position = cls_index
            end_position = cls_index
        else:
            if tokenizer.padding_side == "left":
                doc_offset = 0
            else:
                doc_offset = len(truncated_query) + sequence_added_tokens

            start_position = tok_start_position - doc_start + doc_offset
            end_position = tok_end_position - doc_start + doc_offset

        feature = QAFeatures(
            input_ids=span["input_ids"],
            attention_mask=span["attention_mask"],
            token_type_ids=span["token_type_ids"],
            start_positions=start_position,
            end_positions=end_position,
        )

        features.append(feature)

    return features


def _squad_convert_examples_to_features(
        examples: List[QAExample],
        tokenizer: PreTrainedTokenizer,
        args: TrainArguments,
):
    threads = min(args.threads, cpu_count())
    with Pool(threads, initializer=_squad_convert_example_to_features_init, initargs=(tokenizer,)) as p:
        annotate_ = partial(
            _squad_convert_example_to_features,
            max_seq_length=args.max_seq_length,
            doc_stride=args.doc_stride,
            max_query_length=args.max_query_length,
        )
        features = list(
            tqdm(
                p.imap(annotate_, examples, chunksize=32),
                total=len(examples),
                desc="convert squad examples to features",
                disable=not args.tqdm_enabled,
            )
        )
    new_features = []
    for feature in features:
        if not feature:
            continue
        for f in feature:
            new_features.append(f)
    features = new_features
    del new_features

    return features

class QADataset(Dataset):

    def __init__(
            self,
            args: TrainArguments,
            tokenizer: PreTrainedTokenizer,
            corpus: QACorpus,
            mode: Optional[str] = "train",
            convert_examples_to_features_fn=_squad_convert_examples_to_features,
    ):
        if corpus is not None:
            self.corpus = corpus
        else:
            raise KeyError("corpus is not valid")
        if not mode in ["train", "val", "test"]:
            raise KeyError(f"mode({mode}) is not a valid split name")

        # Load data features from cache or dataset file
        cached_features_file = os.path.join(
            args.downstream_corpus_root_dir,
            args.downstream_corpus_name,
            "cached_{}_{}_{}_{}_{}_{}_{}".format(
                mode,
                tokenizer.__class__.__name__,
                f"maxlen-{args.max_seq_length}",
                f"maxquerylen-{args.max_query_length}",
                f"docstride-{args.doc_stride}",
                args.downstream_corpus_name,
                "question-answering",
            ),
        )

        # Make sure only the first process in distributed training processes the dataset,
        # and the others will use the cache.
        lock_path = cached_features_file + ".lock"
        with FileLock(lock_path):

            if os.path.exists(cached_features_file) and not args.overwrite_cache:
                start = time.time()
                self.features = torch.load(cached_features_file)
         
            else:
                corpus_fpath = os.path.join(
                    args.downstream_corpus_root_dir,
                    args.downstream_corpus_name.lower(),
                )

                examples = self.corpus.get_examples(corpus_fpath, mode)

                self.features = convert_examples_to_features_fn(examples, tokenizer, args)
                start = time.time()
              
                torch.save(self.features, cached_features_file)
             

    def __len__(self):
        return len(self.features)

    def __getitem__(self, i):
        return self.features[i]

